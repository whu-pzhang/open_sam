# model
checkpoint = 'https://download.openmmlab.com/mmsegmentation/v0.5/sam/sam_vit-base-p16_3rdparty_sa1b-1024x1024_20230413-78a25eed.pth'  # noqa
model = dict(
    type='SAM',
    init_cfg=dict(type='Pretrained', checkpoint=checkpoint),
    image_encoder=dict(type='ViTSAM',
                       arch='base',
                       img_size=1024,
                       patch_size=16,
                       out_channels=256,
                       use_abs_pos=True,
                       use_rel_pos=True,
                       window_size=14),
    prompt_encoder=dict(type='PromptEncoder',
                        embed_dim=256,
                        image_embedding_size=(64, 64),
                        input_image_size=(1024, 1024),
                        mask_in_chans=16),
    mask_decoder=dict(type='MaskDecoder',
                      num_multimask_outputs=3,
                      transformer=dict(type='TwoWayTransformer',
                                       depth=2,
                                       embedding_dim=256,
                                       mlp_dim=2048,
                                       num_heads=8),
                      transformer_dim=256,
                      iou_head_depth=3,
                      iou_head_hidden_dim=256),
    #
    # loss_mask=dict(type='mmdet.CrossEntropyLoss',
    #                use_sigmoid=True,
    #                reduction='mean',
    #                loss_weight=1.0),
    loss_mask=dict(type='mmdet.FocalLoss',
                   use_sigmoid=True,
                   gamma=2.0,
                   alpha=0.25,
                   loss_weight=20.0),
    loss_dice=dict(type='mmdet.DiceLoss',
                   use_sigmoid=True,
                   activate=True,
                   reduction='mean',
                   naive_dice=True,
                   eps=1.0,
                   loss_weight=1.0),
    # loss_iou=None,
    #
    train_cfg=dict(
        num_points=12544,
        oversample_ratio=3.0,
        importance_sample_ratio=0.75,
    ),
)
