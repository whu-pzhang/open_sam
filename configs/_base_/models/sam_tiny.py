# model
checkpoint = 'weights/sam_vit-tiny.pth'  # noqa
model = dict(
    type='SAM',
    init_cfg=dict(type='Pretrained', checkpoint=checkpoint),
    image_encoder=dict(type='TinyViT',
                       arch='5m',
                       img_size=1024,
                       out_channels=256,
                       mlp_ratio=4.,
                       drop_rate=0.,
                       drop_path_rate=0.,
                       use_checkpoint=False,
                       mbconv_expand_ratio=4.0,
                       local_conv_size=3,
                       layer_lr_decay=0.8),
    prompt_encoder=dict(type='PromptEncoder',
                        embed_dim=256,
                        image_embedding_size=(64, 64),
                        input_image_size=(1024, 1024),
                        mask_in_chans=16),
    mask_decoder=dict(type='MaskDecoder',
                      num_multimask_outputs=3,
                      transformer=dict(type='TwoWayTransformer',
                                       depth=2,
                                       embedding_dim=256,
                                       mlp_dim=2048,
                                       num_heads=8),
                      transformer_dim=256,
                      iou_head_depth=3,
                      iou_head_hidden_dim=256),
    #
    # loss_mask=dict(type='mmdet.CrossEntropyLoss',
    #                use_sigmoid=True,
    #                reduction='mean',
    #                loss_weight=1.0),
    loss_mask=dict(type='mmdet.FocalLoss',
                   use_sigmoid=True,
                   gamma=2.0,
                   alpha=0.25,
                   loss_weight=20.0),
    loss_dice=dict(type='mmdet.DiceLoss',
                   use_sigmoid=True,
                   activate=True,
                   reduction='mean',
                   naive_dice=True,
                   eps=1.0,
                   loss_weight=1.0),
    # loss_iou=None,
    #
    train_cfg=dict(
        num_points=12544,
        oversample_ratio=3.0,
        importance_sample_ratio=0.75,
    ),
)
