# model
arch = 'base'
checkpoint = 'https://download.openmmlab.com/mmsegmentation/v0.5/sam/sam_vit-base-p16_3rdparty_sa1b-1024x1024_20230413-78a25eed.pth'  # noqa
model = dict(
    type='SAM',
    init_cfg=dict(type='Pretrained', checkpoint=checkpoint),
    image_encoder=dict(type='ViTSAM',
                       arch=arch,
                       img_size=1024,
                       patch_size=16,
                       out_channels=256,
                       use_abs_pos=True,
                       use_rel_pos=True,
                       window_size=14),
    prompt_encoder=dict(type='PromptEncoder',
                        embed_dim=256,
                        image_embedding_size=(64, 64),
                        input_image_size=(1024, 1024),
                        mask_in_chans=16),
    mask_decoder=dict(type='MaskDecoder',
                      num_multimask_outputs=3,
                      transformer=dict(type='TwoWayTransformer',
                                       depth=2,
                                       embedding_dim=256,
                                       mlp_dim=2048,
                                       num_heads=8),
                      transformer_dim=256,
                      iou_head_depth=3,
                      iou_head_hidden_dim=256),
    loss_decode=dict(type='mmseg.CrossEntropyLoss', avg_non_ignore=True),
)
